# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xBe74sx5GBiJyMkPAfD8JrGyeak9Luge
"""

#START
#Importing libraries necessary to preprocess the data
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
import tensorflow
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout
from tensorflow.keras.optimizers import SGD, Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
import cv2
from keras.models import Sequential
from keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout
from keras.callbacks import ReduceLROnPlateau, EarlyStopping

data = pd.read_csv('/content/A_Z Handwritten Data.csv').astype('float32') #Handwritten Data file downloaded form Kaggle
#data.head(15) #Just crosschecking the read command

x = data.drop('0',axis=1) #Image of Character
y = data['0'] #Label of Character #'0' column contains the data labels
print(data.shape) # GIves us no. of rows and columns

#CSV file contained 785-1=784 columns of data. Therefore splitting into 28x29 pixels of image data
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=0) #Splitting Training Data:Test Data = 80:20
x_train = np.reshape(x_train.values, (x_train.shape[0], 28,28)) 
x_test = np.reshape(x_test.values, (x_test.shape[0], 28,28))

print("Shape of Training data: ", x_train.shape)
print("Shape of Testing data: ", x_test.shape)

#Create a Dictionary to store labels of Alphabets
words_dic = {0:'A',1:'B',2:'C',3:'D',4:'E',5:'F',6:'G',7:'H',8:'I',9:'J',10:'K',11:'L',12:'M',13:'N',14:'O',15:'P',16:'Q',17:'R',18:'S',19:'T',20:'U',21:'V',22:'W',23:'X', 24:'Y',25:'Z'}

yno = np.int0(y)
count = np.zeros(26, dtype='int')

for i in yno:
  count[i] +=1

alphabets = []
for i in words_dic.values():
  alphabets.append(i)

fig, ax = plt.subplots(1,1,figsize=(10,10))
ax.barh(alphabets,count)

plt.xlabel("Number of elements ")
plt.ylabel("Alphabets")
plt.grid()
plt.show()

shuffle_data = shuffle(x_train)

fig, axes = plt.subplots(3,3, figsize = (10,10))
axes = axes.flatten()
for i in range(9):
    _, shu = cv2.threshold(shuffle_data[i], 30, 200, cv2.THRESH_BINARY)
    axes[i].imshow(np.reshape(shuffle_data[i], (28,28)), cmap="Greys")
plt.show()

X_train = x_train.reshape(x_train.shape[0],x_train.shape[1],x_train.shape[2],1)
X_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2],1)
print("New shape of training data: ", X_train.shape)
print("New shape of testing data: ", X_test.shape)

#Reshape Train & Test image dataset
y_training = to_categorical(y_train, num_classes = 26, dtype='int')
y_testing = to_categorical(y_test, num_classes = 26, dtype='int')

print("New shape of Training Labels: ", y_training.shape)
print("New shape of Training Labels: ", y_testing.shape)

#Creating a CNN model
#Grouping linear stack of layers
model = Sequential()

#Learning 32 filters
model.add(Conv2D(filters=32, kernel_size=(3,3),activation='relu',input_shape=(28,28,1)))
model.add(MaxPool2D(pool_size=(2,2),strides=2))

model.add(Conv2D(filters=64, kernel_size=(3,3),activation='relu',padding='same'))
model.add(MaxPool2D(pool_size=(2,2),strides=2))

model.add(Conv2D(filters=128, kernel_size=(3,3),activation='relu',padding='valid'))
model.add(MaxPool2D(pool_size=(2,2),strides=2))
#Check whether strided convolution can give better accuracy 

model.add(Flatten())

model.add(Dense(64,activation="relu"))
model.add(Dense(128,activation="relu"))
model.add(Dense(26,activation="softmax")) #Vector of values to probability distribution

#Compiiing
model.compile(optimizer =Adam(learning_rate=0.001),loss='categorical_crossentropy',metrics=['accuracy'])
history=model.fit(X_train,y_training,epochs=1,validation_data=(X_test,y_testing))

model.summary()
model.save(r'Model_HandWr.h5')

#Printing Training and Validation Accuracies/Losses
print("The validation accuracy is :", history.history['val_accuracy'])
print("The training accuracy is :", history.history['accuracy'])
print("The validation loss is :", history.history['val_loss'])
print("The training accuracy is :", history.history['loss'])

#Predicting on Test Data

fig, axes = plt.subplots(3,3, figsize=(8,9))
axes = axes.flatten()

for i,ax in enumerate(axes):
  img = np.reshape(X_test[i],(28,28))
  ax.imshow(img, cmap="Greys")
  pred = words_dic[np.argmax(y_testing[i])]
  ax.set_title("Prediction: "+pred)
  ax.grid()

#Prediction on External Image
eximg = cv2.imread('/content/B.jpg')
eximg_cpy = eximg.copy()

eximg = cv2.cvtColor(eximg,cv2.COLOR_BGR2RGB)
eximg = cv2.resize(eximg, (400,400))

#Processing Copied Image
eximg_cpy = cv2.GaussianBlur(eximg_cpy,(7,7),0)
eximg_gray = cv2.cvtColor(eximg_cpy,cv2.COLOR_BGR2GRAY)
_, eximg_thresh = cv2.threshold(eximg_gray,100,255,cv2.THRESH_BINARY_INV)

eximg_final = cv2.resize(eximg_thresh, (28,28))
eximg_final = np.reshape(eximg_final,(1,28,28,1))

img_pred = words_dic[np.argmax(model.predict(eximg_final))]
cv2.putText(eximg, "Character Predictor ", (20,25),cv2.FONT_HERSHEY_TRIPLEX,0.7,color = (0,0,230))
cv2.putText(eximg,"Prediction: "+img_pred,(50,350),cv2.FONT_HERSHEY_DUPLEX,1.3,color=(255,0,30))
from google.colab.patches import cv2_imshow
cv2_imshow(eximg)

#while(1):
#  k=cv2.waitKey(1)& 0xFF
#  if k==27:
#    break
#cv2.destroyAllWindows()